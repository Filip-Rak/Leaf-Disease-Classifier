/* Querying */
Enter output directory name: EffNetB0_30ep-BS64_LR1e-3-LS5e-2-BA-AMP
/* Initializing */
Using device: cuda:0 (NVIDIA GeForce RTX 3050 Ti Laptop GPU)
/* Training */
Epoch 1, Batch 0, Loss: 1.9228
Epoch 1, Batch 40, Loss: 0.8381
Epoch 1, Batch 80, Loss: 0.8051
Epoch 1, Batch 120, Loss: 0.7003
Epoch 1/30, Loss: 0.8742, Time: 16.42s
Epoch 2, Batch 0, Loss: 0.5753
Epoch 2, Batch 40, Loss: 0.4447
Epoch 2, Batch 80, Loss: 0.4357
Epoch 2, Batch 120, Loss: 0.4365
Epoch 2/30, Loss: 0.4575, Time: 12.27s
Epoch 3, Batch 0, Loss: 0.3926
Epoch 3, Batch 40, Loss: 0.2971
Epoch 3, Batch 80, Loss: 0.4159
Epoch 3, Batch 120, Loss: 0.3478
Epoch 3/30, Loss: 0.3811, Time: 12.17s
Epoch 4, Batch 0, Loss: 0.3626
Epoch 4, Batch 40, Loss: 0.3126
Epoch 4, Batch 80, Loss: 0.2744
Epoch 4, Batch 120, Loss: 0.3310
Epoch 4/30, Loss: 0.3388, Time: 12.04s
Epoch 5, Batch 0, Loss: 0.2881
Epoch 5, Batch 40, Loss: 0.3115
Epoch 5, Batch 80, Loss: 0.2981
Epoch 5, Batch 120, Loss: 0.2953
Epoch 5/30, Loss: 0.3307, Time: 11.99s
Epoch 6, Batch 0, Loss: 0.3112
Epoch 6, Batch 40, Loss: 0.2837
Epoch 6, Batch 80, Loss: 0.2801
Epoch 6, Batch 120, Loss: 0.3440
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 6/30, Loss: 0.3389, Time: 12.18s
Epoch 7, Batch 0, Loss: 0.5379
Epoch 7, Batch 40, Loss: 0.2934
Epoch 7, Batch 80, Loss: 0.4385
Epoch 7, Batch 120, Loss: 0.3208
Epoch 7/30, Loss: 0.3273, Time: 12.01s
Epoch 8, Batch 0, Loss: 0.4136
Epoch 8, Batch 40, Loss: 0.3601
Epoch 8, Batch 80, Loss: 0.2762
Epoch 8, Batch 120, Loss: 0.3051
Epoch 8/30, Loss: 0.3047, Time: 12.18s
Epoch 9, Batch 0, Loss: 0.2893
Epoch 9, Batch 40, Loss: 0.2791
Epoch 9, Batch 80, Loss: 0.2668
Epoch 9, Batch 120, Loss: 0.2693
Epoch 9/30, Loss: 0.3019, Time: 12.03s
Epoch 10, Batch 0, Loss: 0.3198
Epoch 10, Batch 40, Loss: 0.3117
Epoch 10, Batch 80, Loss: 0.2823
Epoch 10, Batch 120, Loss: 0.2701
Epoch 10/30, Loss: 0.3016, Time: 11.99s
Epoch 11, Batch 0, Loss: 0.2710
Epoch 11, Batch 40, Loss: 0.2723
Epoch 11, Batch 80, Loss: 0.2801
Epoch 11, Batch 120, Loss: 0.2896
Epoch 11/30, Loss: 0.2933, Time: 11.98s
Epoch 12, Batch 0, Loss: 0.3290
Epoch 12, Batch 40, Loss: 0.2754
Epoch 12, Batch 80, Loss: 0.4814
Epoch 12, Batch 120, Loss: 0.2949
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 12/30, Loss: 0.3037, Time: 11.98s
Epoch 13, Batch 0, Loss: 0.2732
Epoch 13, Batch 40, Loss: 0.2835
Epoch 13, Batch 80, Loss: 0.2774
Epoch 13, Batch 120, Loss: 0.2723
Epoch 13/30, Loss: 0.2900, Time: 12.05s
Epoch 14, Batch 0, Loss: 0.2768
Epoch 14, Batch 40, Loss: 0.6029
Epoch 14, Batch 80, Loss: 0.2636
Epoch 14, Batch 120, Loss: 0.2863
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 14/30, Loss: 0.2902, Time: 12.11s
Epoch 15, Batch 0, Loss: 0.3351
Epoch 15, Batch 40, Loss: 0.2742
Epoch 15, Batch 80, Loss: 0.2688
Epoch 15, Batch 120, Loss: 0.2650
NOTICE: Validation loss increased in this epoch (2/31).
Epoch 15/30, Loss: 0.2974, Time: 11.99s
Epoch 16, Batch 0, Loss: 0.3636
Epoch 16, Batch 40, Loss: 0.2787
Epoch 16, Batch 80, Loss: 0.2703
Epoch 16, Batch 120, Loss: 0.2775
NOTICE: Validation loss increased in this epoch (3/31).
Epoch 16/30, Loss: 0.2986, Time: 12.00s
Epoch 17, Batch 0, Loss: 0.2961
Epoch 17, Batch 40, Loss: 0.3521
Epoch 17, Batch 80, Loss: 0.2776
Epoch 17, Batch 120, Loss: 0.2614
NOTICE: Validation loss increased in this epoch (4/31).
Epoch 17/30, Loss: 0.2937, Time: 12.00s
Epoch 18, Batch 0, Loss: 0.3482
Epoch 18, Batch 40, Loss: 0.2675
Epoch 18, Batch 80, Loss: 0.2728
Epoch 18, Batch 120, Loss: 0.3083
Epoch 18/30, Loss: 0.2829, Time: 11.99s
Epoch 19, Batch 0, Loss: 0.2629
Epoch 19, Batch 40, Loss: 0.2649
Epoch 19, Batch 80, Loss: 0.2645
Epoch 19, Batch 120, Loss: 0.2659
Epoch 19/30, Loss: 0.2736, Time: 12.07s
Epoch 20, Batch 0, Loss: 0.2755
Epoch 20, Batch 40, Loss: 0.2610
Epoch 20, Batch 80, Loss: 0.2641
Epoch 20, Batch 120, Loss: 0.2612
Epoch 20/30, Loss: 0.2705, Time: 12.08s
Epoch 21, Batch 0, Loss: 0.2887
Epoch 21, Batch 40, Loss: 0.2598
Epoch 21, Batch 80, Loss: 0.2762
Epoch 21, Batch 120, Loss: 0.2610
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 21/30, Loss: 0.2743, Time: 12.13s
Epoch 22, Batch 0, Loss: 0.2620
Epoch 22, Batch 40, Loss: 0.2631
Epoch 22, Batch 80, Loss: 0.2640
Epoch 22, Batch 120, Loss: 0.3103
NOTICE: Validation loss increased in this epoch (2/31).
Epoch 22/30, Loss: 0.2768, Time: 12.49s
Epoch 23, Batch 0, Loss: 0.2666
Epoch 23, Batch 40, Loss: 0.2665
Epoch 23, Batch 80, Loss: 0.2643
Epoch 23, Batch 120, Loss: 0.2643
NOTICE: Validation loss increased in this epoch (3/31).
Epoch 23/30, Loss: 0.2766, Time: 12.33s
Epoch 24, Batch 0, Loss: 0.2630
Epoch 24, Batch 40, Loss: 0.2626
Epoch 24, Batch 80, Loss: 0.3201
Epoch 24, Batch 120, Loss: 0.2654
Epoch 24/30, Loss: 0.2703, Time: 12.30s
Epoch 25, Batch 0, Loss: 0.2621
Epoch 25, Batch 40, Loss: 0.2823
Epoch 25, Batch 80, Loss: 0.2627
Epoch 25, Batch 120, Loss: 0.2592
Epoch 25/30, Loss: 0.2662, Time: 12.05s
Epoch 26, Batch 0, Loss: 0.2576
Epoch 26, Batch 40, Loss: 0.2672
Epoch 26, Batch 80, Loss: 0.2646
Epoch 26, Batch 120, Loss: 0.2586
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 26/30, Loss: 0.2674, Time: 11.96s
Epoch 27, Batch 0, Loss: 0.2623
Epoch 27, Batch 40, Loss: 0.2639
Epoch 27, Batch 80, Loss: 0.2604
Epoch 27, Batch 120, Loss: 0.2605
Epoch 27/30, Loss: 0.2661, Time: 12.02s
Epoch 28, Batch 0, Loss: 0.2652
Epoch 28, Batch 40, Loss: 0.2648
Epoch 28, Batch 80, Loss: 0.2576
Epoch 28, Batch 120, Loss: 0.2784
Epoch 28/30, Loss: 0.2630, Time: 12.16s
Epoch 29, Batch 0, Loss: 0.2600
Epoch 29, Batch 40, Loss: 0.2614
Epoch 29, Batch 80, Loss: 0.2593
Epoch 29, Batch 120, Loss: 0.2615
NOTICE: Validation loss increased in this epoch (1/31).
Epoch 29/30, Loss: 0.2667, Time: 11.99s
Epoch 30, Batch 0, Loss: 0.2605
Epoch 30, Batch 40, Loss: 0.2638
Epoch 30, Batch 80, Loss: 0.3489
Epoch 30, Batch 120, Loss: 0.2608
NOTICE: Validation loss increased in this epoch (2/31).
Epoch 30/30, Loss: 0.2665, Time: 11.99s
Total time spent in training: 366.97s
/* Validation */
Validation Loss: 0.2783, Accuracy: 99.24%
/* Saving */
Model saved to output/EffNetB0_30ep-BS64_LR1e-3-LS5e-2-BA-AMP\model-EffNetB0_30ep-BS64_LR1e-3-LS5e-2-BA-AMP.pth
Confusion matrix saved to output/EffNetB0_30ep-BS64_LR1e-3-LS5e-2-BA-AMP\cm-EffNetB0_30ep-BS64_LR1e-3-LS5e-2-BA-AMP.png
